{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n# import skvideo.io  \nimport os \nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport io\nimport imageio\nimport ipywidgets\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras import backend as K\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-17T09:24:12.770980Z","iopub.execute_input":"2023-10-17T09:24:12.771374Z","iopub.status.idle":"2023-10-17T09:24:27.152945Z","shell.execute_reply.started":"2023-10-17T09:24:12.771343Z","shell.execute_reply":"2023-10-17T09:24:27.151527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install Scikit-Video","metadata":{}},{"cell_type":"code","source":"!pip install scikit-video\nimport skvideo.io","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add surveillance fight dataset from github","metadata":{}},{"cell_type":"code","source":"# !git clone https://github.com/seymanurakti/fight-detection-surv-dataset.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frame_crop_center(video,cropf):\n    f,_,_,_ = video.shape\n    startf = f//2 - cropf//2\n    return video[startf:startf+cropf, :, :, :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load, resize and trim the videos","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm \nimport cv2\n\ndef extract_tarian(path, frame_size, seq_len):\n    list_video = []\n    list_label = []\n    label_index = 0\n    video_dims = []\n    for folder in path:\n        for f in tqdm(os.listdir(folder)):\n            f = os.path.join(folder, f)\n        # checking if it is a file\n            \n            video = skvideo.io.vread(f)\n            video_dims.append(video.shape)\n            L=[]\n\n            #resize video dimensions\n            for i in range(video.shape[0]):\n                frame = cv2.resize(video[i], (frame_size,frame_size), interpolation=cv2.INTER_CUBIC)\n                L.append(frame)\n\n            video = np.asarray(L)\n\n            #center crop video to have consistent video frame number\n            video = frame_crop_center(video, seq_len)\n\n            list_video.append(video)\n            list_label.append(label_index)\n        label_index += 1\n        \n    return list_video, list_label, video_dims","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # images, labels = load_video(path)\nimport h5py\n\nhf = h5py.File(\"/kaggle/input/ucf101-dalam-format-h5/dataset_ucf50_80_15.h5\", 'r')\nvideos, labels = hf['videos'][()], hf['labels'][()]\n\nhf.close()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:25:31.601840Z","iopub.execute_input":"2023-10-17T09:25:31.602564Z","iopub.status.idle":"2023-10-17T09:25:46.094053Z","shell.execute_reply.started":"2023-10-17T09:25:31.602531Z","shell.execute_reply":"2023-10-17T09:25:46.092735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.array(videos).shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:25:47.762057Z","iopub.execute_input":"2023-10-17T09:25:47.762486Z","iopub.status.idle":"2023-10-17T09:25:49.405681Z","shell.execute_reply.started":"2023-10-17T09:25:47.762450Z","shell.execute_reply":"2023-10-17T09:25:49.404424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting seed for reproducibility\nSEED = 77\nos.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\ntf.random.set_seed(SEED)\n\n# DATA\nDATASET_NAME = \"fight/nofights\"\nBATCH_SIZE = 4\nAUTO = tf.data.AUTOTUNE\nFRAME_SIZE = np.array(videos).shape[2]\nSEQ_LEN = np.array(videos).shape[1]\n# INPUT_SHAPE = (3, 60, 60, 3)\nINPUT_SHAPE = (SEQ_LEN, FRAME_SIZE, FRAME_SIZE, 3)\n\n\n# OPTIMIZER\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-5\n\n# TRAINING\nEPOCHS = 50\n\n# TUBELET EMBEDDING\n# PATCH_SIZE = (8, 8, 8)\nPATCH_SIZE = (8, 8, 8)\nNUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n\n# ViViT ARCHITECTURE\nLAYER_NORM_EPS = 1e-6\n# PROJECTION_DIM = 30\nPROJECTION_DIM = 64\nNUM_HEADS = 2\nNUM_LAYERS = 2","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:26:50.181691Z","iopub.execute_input":"2023-10-17T09:26:50.182155Z","iopub.status.idle":"2023-10-17T09:26:52.225230Z","shell.execute_reply.started":"2023-10-17T09:26:50.182123Z","shell.execute_reply":"2023-10-17T09:26:52.223905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Video and Write H5","metadata":{}},{"cell_type":"code","source":"# import gc\n\n# # del images\n# # del labels\n# gc.collect()\n\n# path=[]\n# dir_path = \"/kaggle/input/tarian/tari\"\n# for d in os.listdir(dir_path):\n#     f_path = os.path.join(\"/kaggle/input/tarian/tari\",d)\n#     path.append(f_path)\n# list_video, list_label, video_dims = extract_tarian(path, FRAME_SIZE, SEQ_LEN)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(video_dims)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import h5py\n\n# with h5py.File(\"dataset_tarian.h5\", \"w\") as f:\n#     f.create_dataset(\"images\", data=np.asarray(images))\n#     f.create_dataset(\"labels\", data=np.asarray(labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES = len(np.unique(labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test  = train_test_split(images, labels, test_size=0.2, random_state=1)\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Video duration and dimension analysis ","metadata":{}},{"cell_type":"markdown","source":"The mean number of frames in the videos is 54 which corresponds to around 2 seconds. \nTo make the video duration consistent without losing lots of data the videos are trimmed to have 42 frames in total with a center crop. \n\nUsing a smaller frame number results in poorer performance. ","metadata":{}},{"cell_type":"code","source":"# data= pd.DataFrame(video_dims, columns=['frame_length', 'height', 'width', 'channels'])\n# data.describe()\n\n# del data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aggregate data and create labels ","metadata":{}},{"cell_type":"code","source":"# surv_fights = [video for video in surv_fights if video.shape[0] == 42]\n# surv_no_fights = [video for video in surv_no_fights if video.shape[0] == 42]\n\n# videos = fights + surv_fights + nofights + surv_no_fights\n# videos = np.asarray(videos)\n\n# labels = np.concatenate([np.ones(len(fights)+len(surv_fights)) , np.zeros(len(nofights)+len(surv_no_fights))])\n\n# del fights\n# del nofights\n# del surv_fights\n# del surv_no_fights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [video for video in surv_fights if video.shape[0] == 42]\n# videos, labels = [video, lb for video, lb in zip(list_video, list_label) if video.shape[0] == 42]\n# videos = []\n# labels = []\n# for video,label in zip(list_video,list_label):\n#     if video.shape[0] == SEQ_LEN:\n#         videos.append(video)\n#         labels.append(label)\n# #         print(np.array(video).shape)\n\n# # print(np.array(videos).shape)\n# # print(np.array(labels).shape)\n# videos = np.asarray(videos)\n# labels = np.asarray(labels)\n\n# del list_video\n# del list_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(videos.shape)\n# import h5py\n\n# with h5py.File(\"dataset_tarian\"+str(FRAME_SIZE)+\"_\"+str(SEQ_LEN)+\".h5\", \"w\") as f:\n#     f.create_dataset(\"videos\", data=np.asarray(videos))\n#     f.create_dataset(\"labels\", data=np.asarray(labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train, test, val split ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(videos, labels, test_size=0.2, random_state=2334)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=124567)\n\nprint(X_train.shape)\nprint(X_val.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:27:06.086617Z","iopub.execute_input":"2023-10-17T09:27:06.087025Z","iopub.status.idle":"2023-10-17T09:27:07.977319Z","shell.execute_reply.started":"2023-10-17T09:27:06.086993Z","shell.execute_reply":"2023-10-17T09:27:07.976107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set ViVit model hyperparameters ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess and prepare dataloader","metadata":{}},{"cell_type":"markdown","source":"Takes around 5 ~ 10 minutes to execute","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef preprocess(frames: tf.Tensor, label: tf.Tensor):\n    \"\"\"Preprocess the frames tensors and parse the labels\"\"\"\n    # Preprocess images\n    frames = tf.image.convert_image_dtype(\n        frames[\n            ..., tf.newaxis\n        ],  # The new axis is to help for further processing with Conv3D layers\n        tf.float32,\n    )\n\n    # Parse label\n    label = tf.cast(label, tf.float32)\n    return frames, label\n\n\ndef prepare_dataloader(\n    videos: np.ndarray,\n    labels: np.ndarray,\n    loader_type: str = \"train\",\n    batch_size: int = BATCH_SIZE,\n):\n    \"\"\"Utility function to prepare dataloader\"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n\n    if loader_type == \"train\":\n        dataset = dataset.shuffle(BATCH_SIZE * 2)\n\n    dataloader = (\n        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n        .batch(batch_size)\n        .prefetch(tf.data.AUTOTUNE)\n    )\n\n    return dataloader\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define some model classes ","metadata":{}},{"cell_type":"code","source":"class TubeletEmbedding(layers.Layer):\n    def __init__(self, embed_dim, patch_size, **kwargs):\n        super().__init__(**kwargs)\n        self.projection = layers.Conv3D(\n            filters=embed_dim,\n            kernel_size=patch_size,\n            strides=patch_size,\n            padding=\"VALID\",\n        )\n        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n\n    def call(self, videos):\n        projected_patches = self.projection(videos)\n        flattened_patches = self.flatten(projected_patches)\n        return flattened_patches\n\nclass PositionalEncoder(layers.Layer):\n    def __init__(self, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n\n    def build(self, input_shape):\n        _, num_tokens, _ = input_shape\n        self.position_embedding = layers.Embedding(\n            input_dim=num_tokens, output_dim=self.embed_dim\n        )\n        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n\n    def call(self, encoded_tokens):\n        # Encode the positions and add it to the encoded tokens\n        encoded_positions = self.position_embedding(self.positions)\n        encoded_tokens = encoded_tokens + encoded_positions\n        return encoded_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create model ","metadata":{}},{"cell_type":"code","source":"def create_vivit_classifier(\n    tubelet_embedder,\n    positional_encoder,\n    input_shape=INPUT_SHAPE,\n    transformer_layers=NUM_LAYERS,\n    num_heads=NUM_HEADS,\n    embed_dim=PROJECTION_DIM,\n    layer_norm_eps=LAYER_NORM_EPS,\n    num_classes=NUM_CLASSES,\n):\n    # Get the input layer\n    inputs = layers.Input(shape=input_shape)\n    # Create patches.\n    patches = tubelet_embedder(inputs)\n    # Encode patches.\n    encoded_patches = positional_encoder(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization and MHSA\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n        )(x1, x1)\n\n        # Skip connection\n        x2 = layers.Add()([attention_output, encoded_patches])\n\n        # Layer Normalization and MLP\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        x3 = keras.Sequential(\n            [\n                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n            ]\n        )(x3)\n\n        # Skip connection\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Layer normalization and Global average pooling.\n    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n    representation = layers.GlobalAvgPool1D()(representation)\n\n    # Classify outputs.\n    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define metrics, build, train and save ViVit model","metadata":{}},{"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef run_experiment():\n    # Initialize model\n    model = create_vivit_classifier(\n        tubelet_embedder=TubeletEmbedding(\n            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n        ),\n        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n    )\n\n    # Compile the model with the optimizer, loss function\n    # and the metrics.\n    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(\n        optimizer=optimizer,\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\n            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n            recall_m,\n            precision_m,\n            f1_m,\n        ],\n    )\n\n    # Train the model.\n    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n\n    _, accuracy, recall, precision, f1 = model.evaluate(testloader)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test recall: {round(recall * 100, 2)}%\")\n    print(f\"Test precision: {round(precision * 100, 2)}%\")\n    print(f\"Test F1: {round(f1 * 100, 2)}%\")\n    \n    vivit_scores = [accuracy, recall, precision, f1]\n    return model, vivit_scores\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n\nkfold = KFold(n_splits=5, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_no = 1\nhistory_acc = []\nhistory_pre = []\nhistory_rec = []\nhistory_f1 = []\nhistory_loss = []\nhistories = []\nfor train, test in tqdm(kfold.split(videos, labels)):\n    print(fold_no)\n    model = create_vivit_classifier(\n        tubelet_embedder=TubeletEmbedding(\n            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n        ),\n        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n    )\n\n    # Compile the model with the optimizer, loss function\n    # and the metrics.\n    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(\n        optimizer=optimizer,\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\n            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n            recall_m,\n            precision_m,\n            f1_m,\n        ],\n    )\n    \n    \n    # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n    # Fit data to model\n#     history = model.fit(inputs[train], targets[train],\n#               batch_size=batch_size,\n#               epochs=no_epochs,\n#               verbose=verbosity)\n    \n    X_train, y_train = videos[train], labels[train]\n    trainloader = prepare_dataloader(X_train, y_train, \"train\")\n    \n    \n    \n    \n    X_val, y_val = videos[test], labels[test]\n    testloader = prepare_dataloader(X_val, y_val, \"valid\")\n#     testloader = prepare_dataloader(X_test, y_test, \"test\")\n\n    print(X_train.shape)\n    print(X_val.shape)\n\n    # Train the model.\n    history = model.fit(trainloader, epochs=EPOCHS)\n    histories.append(history.history)\n    _, accuracy, recall, precision, f1 = model.evaluate(testloader)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test recall: {round(recall * 100, 2)}%\")\n    print(f\"Test precision: {round(precision * 100, 2)}%\")\n    print(f\"Test F1: {round(f1 * 100, 2)}%\")\n    \n    vivit_scores = [accuracy, recall, precision, f1]\n    history_acc.append(accuracy)\n    history_pre.append(precision)\n    history_rec.append(recall)\n    history_f1.append(f1)\n    \n    fold_no += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pprint as pp\npp.pprint(histories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model, vivit_scores = run_experiment()\n\nmodel.save('vivit_model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Confusion Matrix","metadata":{}},{"cell_type":"code","source":"y_preds = np.argmax(model.predict(testloader), axis=1)\n\n#Generate the confusion matrix\ncf_matrix = confusion_matrix(y_test, y_preds)\n\nax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n\nax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\nax.set_xlabel('\\nPredicted Values')\nax.set_ylabel('Actual Values ');\n\n## Ticket labels - List must be in alphabetical order\n# ax.xaxis.set_ticklabels(['False','True'])\n# ax.yaxis.set_ticklabels(['False','True'])\n\n## Display the visualization of the Confusion Matrix.\nplt.show()\nax.figure.savefig(\"vivit_cf.png\") \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}